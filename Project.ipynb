{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "### MySQL and MariaDB for Python Developers\n",
    "# Timing and comparing adapter performance\n",
    "\n",
    "In this project, you will work with multiple different MySQL adapters for Python.\n",
    "\n",
    "You will need access to a MySQL installation where you have superuser permissions. If you do not have such access elsewhere, installing to your personal workstation is a good idea.  Alternately, you might wish to use a Docker container for a self-contained installation.  See `https://hub.docker.com/_/mysql` for details on that option. \n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**Create the sample table**\n",
    "\n",
    "This task is not particularly dependent upon the choice of adapter.  You simply want to load in the same sample data that was discussed in the lesson, located in this repository as `data/census-zipcodes-2018.tsv.bz2`.  It is compressed here to make the repository smaller.  Consult the Python `bz2` module or use command line tools to expand it.\n",
    "\n",
    "The next part will work with this table.  Only the fields that exist in the tab-separated file are necessary; we will not use the geometric type that this lesson showed.  As well, you only need to include the square mile measures, which are the canonical ones from the census.  You may wish to use more ordinary names that the codes in the source data, but the columns may be called whatever you like.  Call your table `project_zipcode_geography`.\n",
    "\n",
    "In this part, you should also install all of the adapters discussed in this lesson.  If you wish to try out `trio_mysql` or `tormysql` that is great practice as well.  But at the least, install one asynchronous (e.g. `aiomysql`) and one synchronous adapter (`mysql.connector` or `PyMySQL`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "The solution is largely the same as provided in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "user, pwd, db = 'ine_student', 'ine-password', 'ine'\n",
    "host, port = 'localhost', 3306\n",
    "conn = mysql.connector.connect(database=db, host=host, user=user, password=pwd, port=port)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS project_zipcode_geography;')\n",
    "\n",
    "sql_geography = \"\"\"\n",
    "CREATE TABLE project_zipcode_geography (\n",
    "  zipcode CHAR(5), \n",
    "  land_area NUMERIC(8, 3),  -- largest zips need 5 to left of decimal\n",
    "  water_area NUMERIC(8, 3), -- sizes with 3 digits of precision\n",
    "  lat REAL,      \n",
    "  lon REAL,\n",
    "  PRIMARY KEY (zipcode)\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(sql_geography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 590 ms, total: 2.15 s\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import namedtuple\n",
    "fields = ('zipcode', 'ALAND', 'AWATER', 'land_area', 'water_area', 'lat', 'lon')\n",
    "Data = namedtuple('Data', fields)\n",
    "\n",
    "sql = \"INSERT into project_zipcode_geography VALUES (%s, %s, %s, %s, %s);\"\n",
    "\n",
    "with open('data/census-zipcodes-2018.tsv') as fh:\n",
    "    next(fh)   # discard header line\n",
    "    for line in fh:\n",
    "        row = Data(*line.strip().split('\\t'))\n",
    "        cur.execute(sql, (row.zipcode, row.land_area, row.water_area, row.lat, row.lon))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "**Timing different adapters**\n",
    "\n",
    "For the small data we are working with, all adapters will be very fast.  The speed advantages of asynchronous adapters will not be shown strongly until you work with much larger data sets than this. And especially once you have many queries \"in flight* at the same time. However, your task is to time differences in performance of a relatively expensive query that utilizes nested subqueries.\n",
    "\n",
    "Are you able to measure better performance using `PyMySQL` or `mysql.connector` versus `aiomysql` for this query? Does multi-threading make a difference within syncrhonous adapters? You may wish to write and time scripts at the command line, since within Jupyter, you need to use the trick discussed to nest an asyncio loop within Jupyter's own loop.  More adventurous students should try much larger data sets and more complex queries that they have access to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_nested = \"\"\"\n",
    "SELECT zipcode\n",
    "FROM project_zipcode_geography \n",
    "WHERE 10*water_area > (\n",
    "    SELECT avg(land_area) FROM project_zipcode_geography )\n",
    "AND 2*land_area > (\n",
    "    SELECT avg(water_area) FROM project_zipcode_geography )\n",
    "ORDER BY land_area + water_area;\n",
    "\"\"\"\n",
    "\n",
    "# We need this if we stay inside Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A possible solution**\n",
    "\n",
    "This solution is provided as an inspiration.  More meaningful results require larger data and more complex queries, but those will depend on you obtaining such datasets, and access to PostgreSQL operating under realistic loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import mysql.connector\n",
    "import aiomysql\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query():\n",
    "    conn = await aiomysql.connect(db=db, host=host, user=user, password=pwd, port=port)\n",
    "    cur = await conn.cursor()\n",
    "    await cur.execute(sql_nested)\n",
    "    results = await cur.fetchall()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 1405\n",
      "CPU times: user 20.6 ms, sys: 4.1 ms, total: 24.7 ms\n",
      "Wall time: 65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time   \n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(asyncio.gather(query()))\n",
    "# This nests the results and extra time because of gather()\n",
    "print(f\"Number of results: {len(results[0])}\")                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 1405\n",
      "CPU times: user 10.4 ms, sys: 3.2 ms, total: 13.6 ms\n",
      "Wall time: 53.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "connA = pymysql.connect(database=db, host=host, user=user, password=pwd, port=port)\n",
    "curA = connA.cursor()\n",
    "curA.execute(sql_nested)\n",
    "results = curA.fetchall()\n",
    "print(f\"Number of results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 1405\n",
      "CPU times: user 15.8 ms, sys: 0 ns, total: 15.8 ms\n",
      "Wall time: 54.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "connB = mysql.connector.connect(database=db, host=host, user=user, password=pwd, port=port)\n",
    "curB = connB.cursor()\n",
    "curB.execute(sql_nested)\n",
    "results = curB.fetchall()\n",
    "print(f\"Number of results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
